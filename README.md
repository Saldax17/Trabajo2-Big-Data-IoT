âš™ï¸ Instrucciones de ejecuciÃ³n
ğŸ”¹ OpciÃ³n 1 â€“ EjecuciÃ³n en AWS EMR

Subir el notebook Trabajo #2 - Big Data Definitivo.ipynb a EMR Notebook o Zeppelin.

Ajustar rutas del bucket S3 (raw, trusted, refined).

Ejecutar todas las celdas del flujo ETL y modelo ML.

ğŸ”¹ OpciÃ³n 2 â€“ EjecuciÃ³n en Google Colab

Subir el notebook Trabajo_2_colab_pyspark_Definitivo.ipynb a Colab.

Insertar las credenciales AWS IAM (Access Key y Secret Key).

Ejecutar las celdas de configuraciÃ³n e instalaciÃ³n de Spark.

Confirmar la conexiÃ³n (âœ… SparkSession creada con soporte S3A).

Leer los archivos Parquet desde S3 y realizar el anÃ¡lisis visual.

ğŸ“Š VisualizaciÃ³n y anÃ¡lisis final

La visualizaciÃ³n se realiza sobre los datos refinados (iot_summary) para identificar patrones en las variables ambientales:

import matplotlib.pyplot as plt
df_s3.toPandas().groupby("hour")["CO2"].mean().plot(kind="line", figsize=(8,4))
plt.title("ConcentraciÃ³n promedio de COâ‚‚ por hora del dÃ­a")
plt.xlabel("Hora")
plt.ylabel("COâ‚‚ (ppm)")
plt.grid(True)
plt.show()


Ejemplo de resultados esperados:

Tendencia horaria del COâ‚‚.

CorrelaciÃ³n entre temperatura y ventilaciÃ³n.

DistribuciÃ³n de humedad segÃºn estado de ventilaciÃ³n.

ğŸ“‚ Estructura del repositorio
â”œâ”€â”€ README.md
â”œâ”€â”€ si7006-252-trabajo2-Santiago-Saldarriaga-Saldarriaga.pdf
â”œâ”€â”€ Trabajo #2 - Big Data Definitivo.ipynb          # Notebook usado en EMR
â”œâ”€â”€ Trabajo_2_colab_pyspark_Definitivo.ipynb        # Notebook con conexiÃ³n S3A desde Colab
â”œâ”€â”€ /scripts/                                       # Scripts PySpark o SQL adicionales
â””â”€â”€ /data/                                          # Ejemplos o muestras de datos (si aplica)

ğŸ“ˆ Resultados destacados

ImplementaciÃ³n completa del flujo Batch Big Data en AWS.

ConexiÃ³n validada entre PySpark (Colab) y AWS S3.

Modelo de Random Forest entrenado y almacenado en la zona refined.

Visualizaciones de comportamiento ambiental basadas en datos reales.

IntegraciÃ³n exitosa entre S3, EMR, Glue, Athena y Colab.

ğŸ§¾ Conclusiones

El proyecto demuestra el ciclo completo de procesamiento batch en AWS, integrando componentes analÃ­ticos y de almacenamiento a gran escala.
El uso de PySpark tanto en EMR como en Colab facilita la experimentaciÃ³n y anÃ¡lisis, mientras que el Data Lake asegura escalabilidad y trazabilidad del proceso.

ğŸ¤ CrÃ©ditos

Proyecto desarrollado por Santiago Saldarriaga Saldarriaga
como parte del curso Almacenamiento y Procesamiento de Grandes Datos â€“ Universidad EAFIT (2025-2).

ğŸ“œ Licencia

Este proyecto se distribuye con fines acadÃ©micos bajo la licencia MIT.

