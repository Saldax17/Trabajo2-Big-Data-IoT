# üåé Proyecto 2 ‚Äì Arquitectura Batch Big Data en AWS (EAFIT 2025-2)

**Autor:** Santiago Saldarriaga Saldarriaga  
**Curso:** SI7006 / SI6003 ‚Äì Almacenamiento y Procesamiento de Grandes Datos  
**Fecha de entrega:** 31 de octubre de 2025  

---

## üß† Descripci√≥n general

Este proyecto implementa una **soluci√≥n anal√≠tica batch** en la nube para el monitoreo de la calidad del aire en espacios interiores (como parqueaderos o t√∫neles).  
El objetivo es transformar datos hist√≥ricos en **conocimiento √∫til**, aplicando an√°lisis exploratorio y modelos de aprendizaje autom√°tico sobre un **Data Lake en AWS S3**.

---

## üèóÔ∏è Arquitectura general

La soluci√≥n sigue un **ciclo de vida Batch cl√°sico**, estructurado en seis fases:

1. **Ingesta:**  
   Carga del dataset *IoT Indoor Air Quality* desde Kaggle hacia la zona `raw/` del bucket S3 `ssaldarridatalake2`.

2. **Almacenamiento:**  
   Creaci√≥n de un **Data Lake** en S3 con zonas:
   - `raw/` ‚Äì Datos originales.  
   - `trusted/` ‚Äì Datos limpios y transformados.  
   - `refined/` ‚Äì Resultados del EDA y predicciones del modelo.

3. **Preparaci√≥n:**  
   Limpieza, transformaci√≥n y normalizaci√≥n de los datos mediante **PySpark** en **AWS EMR** o **Google Colab**, generando salidas optimizadas en formato **Parquet**.

4. **Catalogaci√≥n:**  
   Uso de **AWS Glue Crawler** para registrar los datasets procesados dentro de la base `proyecto1db`, habilitando consultas en **Amazon Athena** y **SparkSQL**.

5. **An√°lisis exploratorio (EDA):**  
   C√°lculo de m√©tricas por `ventilation_status`, promedios de temperatura, humedad y CO‚ÇÇ, m√°ximos de PM2.5 y desviaciones est√°ndar, almacenadas en  
   `s3://ssaldarridatalake2/proyecto1/refined/iot_summary/`.

6. **Modelado predictivo:**  
   Entrenamiento de un **Random Forest Classifier** de **SparkML** para predecir el estado de ventilaci√≥n (`On/Off`) a partir de variables ambientales.  
   Resultados guardados en `s3://ssaldarridatalake2/proyecto1/refined/iot_predictions/`.

---

## ‚òÅÔ∏è Componentes AWS utilizados

| Servicio | Rol principal |
|-----------|----------------|
| **S3** | Almacenamiento del Data Lake (`raw`, `trusted`, `refined`). |
| **EMR** | Procesamiento distribuido con PySpark y ejecuci√≥n del modelo ML. |
| **Glue** | Catalogaci√≥n autom√°tica y definici√≥n del esquema de datos. |
| **Athena** | Consulta SQL directa sobre los datos catalogados. |
| **IAM** | Control de accesos y credenciales seguras para EMR / Colab. |

---

## üíª Implementaci√≥n en Google Colab

Se configur√≥ un entorno PySpark en Colab para:

- Conectarse a **S3** mediante `fs.s3a` y credenciales IAM.  
- Leer los datos Parquet desde la zona `refined/`.  
- Ejecutar nuevamente el EDA y el modelo ML.  
- Visualizar resultados mediante `matplotlib` y `seaborn`.

Ejemplo de conexi√≥n:

```python
from pyspark.sql import SparkSession
spark = (SparkSession.builder
         .appName("S3Connection")
         .master("local[*]")
         .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
         .getOrCreate())
