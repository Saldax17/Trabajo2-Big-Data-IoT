# üåé Proyecto 2 ‚Äì Arquitectura Batch Big Data en AWS (EAFIT 2025-2)

**Autor:** Santiago Saldarriaga Saldarriaga  
**Curso:** SI7006 / SI6003 ‚Äì Almacenamiento y Procesamiento de Grandes Datos  
**Fecha de entrega:** 31 de octubre de 2025  

---

## üß† Descripci√≥n general

Este proyecto implementa una **soluci√≥n anal√≠tica batch** en la nube para el monitoreo de la calidad del aire en espacios interiores (por ejemplo, parqueaderos o t√∫neles).  
El objetivo principal es transformar datos hist√≥ricos de sensores IoT en **conocimiento √∫til**, aplicando an√°lisis exploratorio y modelos de aprendizaje autom√°tico sobre un **Data Lake en AWS S3**.

---

## üèóÔ∏è Arquitectura general

La soluci√≥n se dise√±√≥ siguiendo el flujo **Batch Architecture**, con separaci√≥n por zonas en el Data Lake y herramientas de AWS:

1. **Ingesta:**  
   Carga del dataset *IoT Indoor Air Quality* desde Kaggle hacia la zona `raw/` del bucket S3 `ssaldarridatalake2`.

2. **Almacenamiento:**  
   Creaci√≥n de un **Data Lake** con las zonas:
   - `raw/` ‚Üí Datos originales sin procesar.  
   - `trusted/` ‚Üí Datos limpios y transformados.  
   - `refined/` ‚Üí Resultados finales del an√°lisis y modelo ML.

3. **Preparaci√≥n:**  
   Limpieza, transformaci√≥n y normalizaci√≥n de los datos con **PySpark** ejecutado en **AWS EMR** o **Google Colab**, generando salidas optimizadas en formato **Parquet**.

4. **Catalogaci√≥n:**  
   Uso de **AWS Glue Crawler** para registrar los datasets en la base `proyecto1db` y habilitar consultas con **Amazon Athena** y **SparkSQL**.

5. **An√°lisis Exploratorio (EDA):**  
   C√°lculo de m√©tricas ambientales (promedios, m√°ximos, desviaciones) por hora y por estado de ventilaci√≥n, almacenadas en  
   `s3://ssaldarridatalake2/proyecto1/refined/iot_summary/`.

6. **Modelado Predictivo:**  
   Entrenamiento de un modelo **Random Forest Classifier** con **SparkML** para predecir el estado de ventilaci√≥n (`ventilation_status`) en funci√≥n de variables ambientales.  
   Resultados almacenados en `s3://ssaldarridatalake2/proyecto1/refined/iot_predictions/`.

---

## ‚òÅÔ∏è Componentes AWS utilizados

| Servicio | Funcionalidad |
|-----------|----------------|
| **Amazon S3** | Data Lake con zonas `raw`, `trusted` y `refined`. |
| **Amazon EMR** | Ejecuci√≥n distribuida de PySpark y entrenamiento del modelo ML. |
| **AWS Glue** | Descubrimiento y catalogaci√≥n autom√°tica de los datos procesados. |
| **Amazon Athena** | Consulta SQL sobre datos en formato Parquet. |
| **IAM** | Gesti√≥n de credenciales seguras para acceso desde EMR y Colab. |

---

## üíª Implementaci√≥n en Google Colab

En Colab se configur√≥ un entorno **PySpark** con acceso directo a **S3** mediante el conector `s3a://`.  
Este entorno permiti√≥ realizar an√°lisis exploratorio, modelado y visualizaci√≥n directamente desde la nube.

### üîß Configuraci√≥n resumida de la conexi√≥n
```python
from pyspark.sql import SparkSession
import os

os.environ["AWS_ACCESS_KEY_ID"] = "TU_ACCESS_KEY"
os.environ["AWS_SECRET_ACCESS_KEY"] = "TU_SECRET_KEY"
os.environ["AWS_REGION"] = "us-west-1"

spark = (
    SparkSession.builder
    .appName("S3Connection")
    .master("local[*]")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config("spark.hadoop.fs.s3a.aws.credentials.provider",
            "com.amazonaws.auth.EnvironmentVariableCredentialsProvider")
    .config("spark.hadoop.fs.s3a.endpoint", "s3.us-west-1.amazonaws.com")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .getOrCreate()
)
